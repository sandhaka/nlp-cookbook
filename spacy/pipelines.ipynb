{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence\n"
     ]
    }
   ],
   "source": [
    "# Doc object\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence\")\n",
    "print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:17.010452Z",
     "end_time": "2023-04-24T19:55:20.162955Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pipeline components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tagger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This: \n",
      "[.tag] 15267657372422890137, part-of-speech tag Id\n",
      "[.pos] 95, position in the sentence\n",
      "\n",
      "is: \n",
      "[.tag] 13927759927860985106, part-of-speech tag Id\n",
      "[.pos] 87, position in the sentence\n",
      "\n",
      "a: \n",
      "[.tag] 15267657372422890137, part-of-speech tag Id\n",
      "[.pos] 90, position in the sentence\n",
      "\n",
      "sentence: \n",
      "[.tag] 15308085513773655218, part-of-speech tag Id\n",
      "[.pos] 92, position in the sentence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text}: ')\n",
    "    print(f'[.tag] {token.tag}, part-of-speech tag Id')\n",
    "    print(f'[.pos] {token.pos}, position in the sentence')\n",
    "    print(\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.164069Z",
     "end_time": "2023-04-24T19:55:20.166098Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Parser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This: \n",
      "[.head] is, syntactic parent of the token, if hase one\n",
      "[.dep] 429, syntactic dependency id\n",
      "[.sent] This is a sentence, relative sentence span\n",
      "\n",
      "is: \n",
      "[.head] is, syntactic parent of the token, if hase one\n",
      "[.dep] 8206900633647566924, syntactic dependency id\n",
      "[.sent] This is a sentence, relative sentence span\n",
      "\n",
      "a: \n",
      "[.head] sentence, syntactic parent of the token, if hase one\n",
      "[.dep] 415, syntactic dependency id\n",
      "[.sent] This is a sentence, relative sentence span\n",
      "\n",
      "sentence: \n",
      "[.head] is, syntactic parent of the token, if hase one\n",
      "[.dep] 404, syntactic dependency id\n",
      "[.sent] This is a sentence, relative sentence span\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text}: ')\n",
    "    print(f'[.head] {token.head if token.has_head() else None}, syntactic parent of the token, if hase one')\n",
    "    print(f'[.dep] {token.dep}, syntactic dependency id')\n",
    "    print(f'[.sent] {token.sent}, relative sentence span')\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.166570Z",
     "end_time": "2023-04-24T19:55:20.168268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Named Entity recognizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text}: ')\n",
    "    print(f'{ent}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.169667Z",
     "end_time": "2023-04-24T19:55:20.171018Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Text classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "for c in doc.cats:\n",
    "    print(f'{c}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.171324Z",
     "end_time": "2023-04-24T19:55:20.172571Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because text categories are always very specific, the text classifier is not included in any of the trained pipelines by default. But you can use it to train your own system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All pipeline packages you can load into spaCy include several files and a config.cfg.\n",
    "\n",
    "The config defines things like the language and pipeline. This tells spaCy which components to instantiate and how they should be configured.\n",
    "\n",
    "The built-in components that make predictions also need binary data. The data is included in the pipeline package and loaded into the component when you load the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.173192Z",
     "end_time": "2023-04-24T19:55:20.174876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x14fe95c70>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x14fe95670>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x14f8373e0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x14ff1f810>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x14ff29810>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x14f8374c0>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.175177Z",
     "end_time": "2023-04-24T19:55:20.176671Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Custom pipeline components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.177133Z",
     "end_time": "2023-04-24T19:55:20.178910Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the text is tokenized and a Doc object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own.\n",
    "\n",
    "```python\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(d):\n",
    "    # Do something to the doc here\n",
    "    return d\n",
    "\n",
    "nlp.add_pipe(\"custom_component\", last=True) # Default\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(d):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(d))\n",
    "    # Return the doc object\n",
    "    return d\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "_ = nlp(\"Hello world!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.179707Z",
     "end_time": "2023-04-24T19:55:20.457634Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(d):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(d)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(d, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    d.ents = spans\n",
    "    return d\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.460269Z",
     "end_time": "2023-04-24T19:55:20.791122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension(\"is_color\", getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"The lake is blue\")\n",
    "\n",
    "print(doc[3]._.is_color)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.792088Z",
     "end_time": "2023-04-24T19:55:20.796833Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.797921Z",
     "end_time": "2023-04-24T19:55:20.803041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return f\"https://en.wikipedia.org/w/index.php?search={entity_text}\"\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", method=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:20.804189Z",
     "end_time": "2023-04-24T19:55:21.084978Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you need to process a lot of texts and create a lot of Doc objects in a row, the nlp.pipe method can speed this up significantly. It processes the texts as a stream and yields Doc objects. It is much faster than just calling nlp on each text, because it batches up the texts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T19:55:21.086176Z",
     "end_time": "2023-04-24T19:55:21.091055Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
